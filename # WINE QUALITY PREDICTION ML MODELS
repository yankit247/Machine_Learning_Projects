import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# loading the dataset to a Pandas DataFrame
wine_dataset = pd.read_csv('/content/winequality-red.csv')

# number of rows & columns in the dataset
wine_dataset.shape

# first 5 rows of the dataset
wine_dataset.head()

# checking for missing values
wine_dataset.isnull().sum()

# statistical measures of the dataset
wine_dataset.describe()

# number of values for each quality
sns.catplot(x='quality', data = wine_dataset, kind = 'count')

# volatile acidity vs Quality
plot = plt.figure(figsize=(5,5))
sns.barplot(x='quality', y = 'volatile acidity', data = wine_dataset)

# citric acid vs Quality
plot = plt.figure(figsize=(5,5))
sns.barplot(x='quality', y = 'citric acid', data = wine_dataset)

correlation = wine_dataset.corr()

# constructing a heatmap to understand the correlation between the columns
plt.figure(figsize=(10,10))
sns.heatmap(correlation, cbar=True, square=True, fmt = '.1f', annot = True, annot_kws={'size':8}, cmap = 'Blues')

# separate the data and Label
X = wine_dataset.drop('quality',axis=1)

print(X)
Y = wine_dataset['quality'].apply(lambda y_value: 1 if y_value>=7 else 0)

print(Y)

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=3)
print(Y.shape, Y_train.shape, Y_test.shape)

model = RandomForestClassifier()
model.fit(X_train, Y_train)

# accuracy on test data
X_test_prediction = model.predict(X_test)
test_data_accuracy = accuracy_score(X_test_prediction, Y_test)

print('Accuracy : ', test_data_accuracy)



import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Load the dataset
wine_dataset = pd.read_csv('/content/winequality-red.csv')

# Separate features and labels
X = wine_dataset.drop('quality', axis=1)
Y = wine_dataset['quality'].apply(lambda y: 1 if y >= 7 else 0)

# Split the dataset into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=3)

# Create and train the Logistic Regression model
logistic_model = LogisticRegression()
logistic_model.fit(X_train, Y_train)

# Create and train the Decision Tree Classifier model
decision_tree_model = DecisionTreeClassifier()
decision_tree_model.fit(X_train, Y_train)

# Create and train the Support Vector Classifier (SVC) model
svc_model = SVC()
svc_model.fit(X_train, Y_train)

# Make predictions on the test set
Y_pred_logistic = logistic_model.predict(X_test)
Y_pred_decision_tree = decision_tree_model.predict(X_test)
Y_pred_svc = svc_model.predict(X_test)

# Calculate the accuracy of each model
accuracy_logistic = accuracy_score(Y_test, Y_pred_logistic)
accuracy_decision_tree = accuracy_score(Y_test, Y_pred_decision_tree)
accuracy_svc = accuracy_score(Y_test, Y_pred_svc)

# Print the accuracies
print('Logistic Regression Accuracy:', accuracy_logistic)
print('Decision Tree Classifier Accuracy:', accuracy_decision_tree)
print('Support Vector Classifier (SVC) Accuracy:', accuracy_svc)
